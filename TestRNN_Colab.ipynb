{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TestRNN-Colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd2Jfzsn9aRx"
      },
      "source": [
        "txt = \"Le saumon est « anadrome » (migrateur pour se reproduire), amphibiotique (adapté à la vie dans deux milieux aquatiques), potamotoque (il se reproduit en rivière) et thalassotrophe (il grandit en mer) : il naît en eau douce en eaux courantes près des sources, puis descend instinctivement jusqu'à la mer où il vit 1 à 3 ans, puis retourne dans le fleuve dans lequel il est né (phénomène dénommé « Homing ») pour frayer (se reproduire) et généralement mourir après la ponte (certaines populations de quelques espèces peuvent cependant passer toute leur vie en eau douce).Ce cycle implique de profondes modifications physiologiques permettant une adaptation au large gradient de salinité auquel chaque individu doit s'adapter de sa naissance à sa mort. Il implique aussi une capacité (hormonale et de perception des modifications environnementales) lui permettant de migrer à la saison convenant le mieux à la « montaison » et à la reproduction8. Le suivi de biomarqueurs de stress chez des populations différentes remontant des cours d'eau différents montre des différences entre populations, avec un niveau de stress souvent corrélé avec le taux d'échecs dans la montaison et à la mortalité lors de celle-ci.Les reproducteurs meurent habituellement après la ponte, mais quelques mâles du saumon royal ou saumon chinook tout comme le saumon atlantique (Salmo salar) retournent en mer et participent une seconde fois à la reproduction. Poussé par son instinct, chaque saumon parcourt des milliers de kilomètres et remonte même de tout petits ruisseaux. Certains franchissent des cascades de trois mètres ou traversent des routes en profitant des inondations. Même en l'absence d'obstacle physique et hors de la prédation naturelle, de nombreux poissons meurent durant la remontée, probablement parce qu'affaiblis ou perturbés par la pollution de l'eau, en raison d'une pollution génétique (croisement avec des saumons d'élevages qui se sont enfuis dans la nature) et/ou en raison de difficultés de régulation osmotique. Une fois sur le lieu de ponte (la frayère), la femelle creuse des dépressions dans le gravier avec sa queue. Quand elle pond, le mâle émet son sperme. Les saumons forment des couples, le mâle cherchant à éloigner les autres mâles de la femelle. La femelle recouvre ensuite les œufs de graviers, les mettant ainsi à l'abri des prédateurs, avant de mourir (comme le mâle en général). Les œufs pondus à l'automne passent l'hiver dans le gravier, oxygénés par l'eau froide et courante. L'éclosion a lieu en mars ou en avril, selon la température. Les alevins s'enfouissent alors un peu plus profondément dans le gravier, ce qui leur évite d'être emportés lors de la débâcle printanière. Ils y demeurent 5 à 6 semaines, se nourrissant du contenu de leur sac vitellin. Fin avril, début mai, les alevins émergent du gravier et commencent à s'alimenter de plancton et larves d'insectes. Ils fréquentent les endroits où la rivière est peu profonde et le courant important (radiers, sub-affleurements…). Ils profitent alors de la nourriture indirectement issue du « recyclage » des cadavres (nécromasse) de leurs géniteurs. Les bactéries et microchampignons prolifèrent en biofilms riches en oligoéléments rapportés de la mer (dont iode, qui eux-mêmes alimentent des microinvertébrés et/ou des macroinvertébrés dulcicoles qui seront la nourriture des alevins12. Les cadavres de saumons géniteurs étaient autrefois si nombreux que les vertébrés nécrophages ne pouvaient en consommer qu'une petite partie. On a comparé en Alaska le biofilm naturel et la biomasse de macroinvertébrés d'un cours d'eau où étaient venus pondre environ 75 000 saumons adultes et une partie du cours d'eau situé en amont de la frayère. En aval de cette dernière et après la mort des reproducteurs, la masse sèche de biofilm était 15 fois plus élevée qu'en amont de la frayère12, et la densité totale en macroinvertébrés était jusqu'à 25 fois supérieure dans les zones enrichie par les cadavres de saumons12. Dans ce cas, (saumons morts à demi-immergés dans une eau peu profonde et bien oxygénée), ces macroinvertébrés benthiques d'eau douce étaient principalement des moucherons chironomidés, des éphémères (Baetis et Cinygmula) ainsi que des perles. À la fin du premier été, les alevins mesurent environ 5 cm et sont nommés « tacons » ; très semblables physiquement à leurs cousines les truitelles, qui fréquentent les mêmes habitats. Après un à deux ans les jeunes saumons d'environ 15 cm sont prêts à s'en aller en mer. Il semblerait que c'est à ce moment, durant la smoltification (acquisition de la capacité à vivre en milieu salé) que le saumoneau mémorise l'odeur et le goût de sa rivière. Lors des crues du printemps les pré-smolts ou smolts dévalent vers la mer. Certains, trop en retard, n'iront pas au-delà de l'estuaire, leur capacité à vivre en mer ayant disparu, ils resteront en eau douce une année supplémentaire et partiront enfin en mer en temps opportun. Les juvéniles peuvent arriver relativement précocement en mer (ils ne pèsent alors que 0,3 g) avant même le plein développement de leurs adaptations physiologique à la vie en mer (par rapport à d'autres salmonidés anadromes). Ils vivent alors plutôt dans les deux premiers mètres de la colonne d'eau (eaux souvent un peu moins salées en aval des estuaires)13. Ils sont alors très voraces et grandissent rapidement (jusqu'à un doublement mensuel de sa masse corporelle chez le saumon rose en mer les deux premiers mois, après quoi le saumon est parfaitement adapté à la vie en mer). Le juvénile est habituellement très résilient aux maladies infectieuses et même aux parasitoses par le pou du saumon, dont il se débarrasse facilement aux stades copépodites14 (4e mue du pou du saumon). Les saumons sont capables de parcourir des centaines de kilomètres en remontant des rivières. En France, le Salmo salar atlantique de Loire-Allier parcourt presque 1 000 km pour atteindre les frayères du Haut-Allier). La construction de grands barrages modernes a coupé de nombreux cours d'eau, mais des échelles à saumon ont peu à peu été installées pour permettre aux migrateurs de franchir ces obstacles. Une mortalité par épuisement à cause d'une mauvaise qualité de l'eau et d'obstacles encore trop difficiles à franchir (et parfois d'une faible profondeur d'eau à l'approche des frayères) est notablement élevée ; dans la nature et plus encore dans certains cours d'eau artificialisés, ceux qui réussissent à remonter sont souvent blessés (bouche, abdomen...). Dans les zones sauvages nord-américaines, la prédation par les ours, lynx, loups, aigles pêcheurs et autres animaux lors de la remontée était également autrefois très importante, mais elle restait très faible au regard du nombre total de géniteurs. Elle jouait probablement un rôle en matière de sélection naturelle.La Lune, ou Terre I est l'unique satellite naturel permanent de la planète Terre. Il s'agit du cinquième plus grand satellite naturel du Système solaire et du plus grand des satellites planétaires par rapport à la taille de la planète autour de laquelle il orbite. Elle est le deuxième satellite le plus dense du Système solaire après Io, un satellite de Jupiter. La Lune est en rotation synchrone avec la Terre, lui montrant donc constamment la même face. Celle-ci, appelée face visible, est marquée par des mers lunaires volcaniques sombres qui remplissent les espaces entre les hautes terres claires — certaines atteignant 9 km d'altitude — et ses cratères d'impact proéminents. Réciproquement, elle possède une face cachée, qui présente moins de mers mais beaucoup plus de cratères, dont le bassin Pôle Sud-Aitken, le plus grand du satellite et l'un des plus grands du Système solaire par son diamètre de 2 500 km. Elle est dépourvue d'atmosphère dense et de champ magnétique. Son influence gravitationnelle sur la Terre produit les marées océaniques, les marées terrestres, un léger allongement de la durée du jour et la stabilisation de l'inclinaison de l'axe terrestre. La distance orbitale moyenne de la Lune est de 384 402 km, soit environ trente fois le diamètre terrestre, et sa période de révolution vaut 27,3 jours. La taille apparente de la Lune dans le ciel est approximativement la même que celle du Soleil, puisque le diamètre de l'étoile est environ 400 fois celui du satellite, mais qu'elle est également 400 fois plus éloignée. Par conséquent, la Lune peut couvrir presque exactement le Soleil dans le ciel, permettant l'apparition d'éclipses solaires totales. Cette correspondance de taille apparente disparaîtra dans un avenir lointain du fait de l'augmentation de la distance lunaire d'environ 3,8 cm par an. La formation de la Lune remonterait à il y a environ 4,51 milliards d'années, peu de temps après celle de la Terre. L'explication la plus largement acceptée est que la Lune s'est formée à partir des débris restants après un impact géant entre une proto-Terre et une protoplanète de la taille de Mars, appelée Théia. Le satellite naturel est survolé pour la première fois par la sonde spatiale Luna 2 en 1959. Durant plus d'une décennie, elle est notamment étudiée par les programmes Luna et Apollo, respectivement soviétique et américain. Cette course à l'espace culmine en 1969 avec les premiers humains posant le pied sur la Lune lors de la mission Apollo 11 emportant Neil Armstrong et Buzz Aldrin. Dix autres astronautes de la NASA foulent ensuite le sol lunaire jusqu'à Apollo 17 en 1972. Ces missions permettent de ramener sur Terre des roches lunaires qui, avec les observations effectuées sur place, permettent de développer la connaissance géologique de la Lune, de sa structure interne et de l'histoire de sa formation. Délaissée à partir de 1974 par les puissances spatiales, l'astre connaît un nouvel intérêt dans les années 1990, deux missions de la NASA — Clementine et Lunar Prospector — découvrant des indices de la présence de glace d'eau, notamment au pôle Sud. À compter de la fin des années 1990, la Lune est la destination principale des sondes spatiales des nouvelles nations spatiales, notamment la Chine, le Japon et l'Inde. De nouvelles missions habitées vers la Lune, voire une colonisation, sont envisagées dans les années 2020.En sa qualité de deuxième objet céleste dans le ciel terrestre par sa magnitude apparente, après le Soleil, et du fait de son cycle régulier de phases correspondant à sa période synodique de 29,5 jours, la Lune sert de référence et d'influence culturelle aux sociétés humaines depuis des temps immémoriaux. Celles-ci se retrouvent dans la langue, les calendriers, l'art et la mythologie. Par exemple, la déesse Luna, dans la mythologie romaine, ou Séléné, dans la mythologie grecque, ont donné respectivement son nom et un adjectif correspondant.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do0Y1z1M9lrJ",
        "outputId": "762644fb-91ea-42a8-a851-ce04d9e71202"
      },
      "source": [
        "# -*-coding:utf-8 -*-\r\n",
        "\r\n",
        "#tutorial from : https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/\r\n",
        "\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "import numpy as np\r\n",
        "import time\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\r\n",
        "is_cuda = torch.cuda.is_available()\r\n",
        "\r\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\r\n",
        "if is_cuda:\r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "    print(\"GPU is available\")\r\n",
        "else:\r\n",
        "    device = torch.device(\"cpu\")\r\n",
        "    print(\"GPU not available, CPU used\")\r\n",
        "\r\n",
        "\r\n",
        "taille = 100 #length of a sequence # de 100 à 200 généralement (50 pk pas)\r\n",
        "batch_len = 64 # 16 ou 32\r\n",
        "id_last_car = len(txt)-taille #index of the last character to parse for creating the sequences\r\n",
        "text = []\r\n",
        "\r\n",
        "\r\n",
        "for i in range(id_last_car):\r\n",
        "    text.append(txt[i:i+taille+1])\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "for i in range(0, id_last_car, taille+1):\r\n",
        "    text.append(txt[i:i+taille+1])\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "\r\n",
        "# Join all the sentences together and extract the unique characters from the combined sentences\r\n",
        "chars = set(''.join(text))\r\n",
        "\r\n",
        "# Creating a dictionary that maps integers to the characters\r\n",
        "int2char = dict(enumerate(chars))\r\n",
        "\r\n",
        "# Creating another dictionary that maps characters to integers\r\n",
        "char2int = {char: ind for ind, char in int2char.items()}\r\n",
        "\r\n",
        "\r\n",
        "# Finding the length of the longest string in our data\r\n",
        "maxlen = len(max(text, key=len))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# Creating lists that will hold our input and target sequences\r\n",
        "input_seq = []\r\n",
        "target_seq = []\r\n",
        "\r\n",
        "for i in range(len(text)):\r\n",
        "    # Remove last character for input sequence\r\n",
        "  input_seq.append(text[i][:-1])\r\n",
        "    \r\n",
        "    # Remove first character for target sequence\r\n",
        "  target_seq.append(text[i][1:])\r\n",
        "  #print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))\r\n",
        "\r\n",
        "\r\n",
        "for i in range(len(text)):\r\n",
        "    input_seq[i] = [char2int[character] for character in input_seq[i]]\r\n",
        "    target_seq[i] = [char2int[character] for character in target_seq[i]]\r\n",
        "\r\n",
        "\r\n",
        "dict_size = len(char2int)\r\n",
        "seq_len = maxlen - 1\r\n",
        "batch_size = len(text)\r\n",
        "\r\n",
        "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\r\n",
        "    # Creating a multi-dimensional array of zeros with the desired output shape\r\n",
        "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\r\n",
        "    \r\n",
        "    # Replacing the 0 at the relevant character index with a 1 to represent that character\r\n",
        "    for i in range(batch_size):\r\n",
        "        for u in range(seq_len):\r\n",
        "            features[i, u, sequence[i][u]] = 1\r\n",
        "    return features\r\n",
        "\r\n",
        "# Input shape --> (Batch Size, Sequence Length, One-Hot Encoding Size)\r\n",
        "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)\r\n",
        "\r\n",
        "input_seq = torch.from_numpy(input_seq)\r\n",
        "target_seq = torch.Tensor(target_seq)\r\n",
        "\r\n",
        "\r\n",
        "#return nb_samples samples from input_t & target_t\r\n",
        "def sample_seq(nb_samples, total, input_t, target_t):\r\n",
        "    global new_input, new_target, indexes\r\n",
        "    indexes = np.random.randint(0, total-1, nb_samples)\r\n",
        "    input_shape = input_seq.size()\r\n",
        "    target_shape = target_seq.size()\r\n",
        "    new_input = torch.randn((nb_samples, input_shape[1], input_shape[2]), dtype=torch.float32)\r\n",
        "    new_target = torch.randn((nb_samples, target_shape[1]), dtype=torch.float32)\r\n",
        "\r\n",
        "    for a in range(len(indexes)):\r\n",
        "        new_input[a] = input_seq[indexes[a]].detach().clone()\r\n",
        "        new_target[a] = target_seq[indexes[a]].detach().clone()\r\n",
        "    return new_input, new_target\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class Model(nn.Module):\r\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\r\n",
        "        super(Model, self).__init__()\r\n",
        "\r\n",
        "        # Defining some parameters\r\n",
        "        self.hidden_dim = hidden_dim\r\n",
        "        self.n_layers = n_layers\r\n",
        "\r\n",
        "        #Defining the layers\r\n",
        "        # RNN Layer\r\n",
        "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   #essayer GRU (ou LSTM)\r\n",
        "        # Fully connected layer\r\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\r\n",
        "    \r\n",
        "    def forward(self, x):\r\n",
        "        \r\n",
        "        batch_size = x.size(0)\r\n",
        "\r\n",
        "        # Initializing hidden state for first input using method defined below\r\n",
        "        hidden = self.init_hidden(batch_size)\r\n",
        "\r\n",
        "        # Passing in the input and hidden state into the model and obtaining outputs\r\n",
        "        out, hidden = self.rnn(x, hidden)\r\n",
        "        \r\n",
        "        # Reshaping the outputs such that it can be fit into the fully connected layer\r\n",
        "        out = out.contiguous().view(-1, self.hidden_dim)\r\n",
        "        out = self.fc(out)\r\n",
        "        \r\n",
        "        return out, hidden\r\n",
        "    \r\n",
        "    def init_hidden(self, batch_size):\r\n",
        "        # This method generates the first hidden state of zeros which we'll use in the forward pass\r\n",
        "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\r\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\r\n",
        "        return hidden\r\n",
        "\r\n",
        "\r\n",
        "# This function takes in the model and character as arguments and returns the next character prediction and hidden state\r\n",
        "def predict(model, character):\r\n",
        "    # One-hot encoding our input to fit into the model\r\n",
        "    character = np.array([[char2int[c] for c in character]])\r\n",
        "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\r\n",
        "    character = torch.from_numpy(character)\r\n",
        "    character = character.to(device)\r\n",
        "    \r\n",
        "    out, hidden = model(character)\r\n",
        "\r\n",
        "    prob = nn.functional.softmax(out[-1], dim=0).data\r\n",
        "    # Taking the class with the highest probability score from the output\r\n",
        "    char_ind = torch.max(prob, dim=0)[1].item()\r\n",
        "\r\n",
        "    return int2char[char_ind], hidden\r\n",
        "\r\n",
        "\r\n",
        "# This function takes the desired output length and input characters as arguments, returning the produced sentence\r\n",
        "def sample(model, out_len, start):\r\n",
        "    model.eval() # eval mode\r\n",
        "    ## start = start.lower() # WHY ?\r\n",
        "    # First off, run through the starting characters\r\n",
        "    chars = [ch for ch in start]\r\n",
        "    size = out_len - len(chars)\r\n",
        "    # Now pass in the previous characters and get a new one\r\n",
        "    for ii in range(size):\r\n",
        "        char, h = predict(model, chars)\r\n",
        "        chars.append(char)\r\n",
        "\r\n",
        "    return ''.join(chars)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxIkLZ3s9kRW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zzaxRZU4-NeQ",
        "outputId": "99bc0661-cc76-44ec-d7d0-0866f40eb91d"
      },
      "source": [
        "# Instantiate the model with hyperparameters\r\n",
        "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=512, n_layers=1)\r\n",
        "# We'll also set the model to the device that we defined earlier\r\n",
        "model = model.to(device)\r\n",
        "\r\n",
        "# set the input_seq ant target_seq to the device used\r\n",
        "input_seq = input_seq.to(device) \r\n",
        "target_seq = target_seq.to(device)\r\n",
        "\r\n",
        "\r\n",
        "# Define hyperparameters\r\n",
        "n_epochs = 5000\r\n",
        "lr=0.001\r\n",
        "\r\n",
        "\r\n",
        "# Define Loss, Optimizer\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\r\n",
        "\r\n",
        "\r\n",
        "# Training Run\r\n",
        "t1 = time.time()\r\n",
        "for epoch in range(1, n_epochs + 1):\r\n",
        "    optimizer.zero_grad() # Clears existing gradients from previous epoch\r\n",
        "    #input_sample, target_sample = sample_seq(batch_len, batch_size, input_seq, target_seq)\r\n",
        "    output, hidden = model(input_seq)\r\n",
        "    loss = criterion(output, target_seq.view(-1).long())\r\n",
        "\r\n",
        "    loss.backward() # Does backpropagation and calculates gradients\r\n",
        "    optimizer.step() # Updates the weights accordingly\r\n",
        "    \r\n",
        "    if epoch%10 == 0:\r\n",
        "        t2 = time.time()-t1\r\n",
        "        t1 = time.time()\r\n",
        "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\r\n",
        "        print(\"Loss: {:.4f}   {:.4f}\".format(loss.item(), t2))\r\n",
        "'''\r\n",
        "loss = 1000\r\n",
        "epoch = 1\r\n",
        "t1 = time.time()\r\n",
        "while loss > 0.3:\r\n",
        "    optimizer.zero_grad() # Clears existing gradients from previous epoch\r\n",
        "    input_sample, target_sample = sample_seq(batch_len, batch_size, input_seq, target_seq)\r\n",
        "    output, hidden = model(input_seq)\r\n",
        "    loss = criterion(output, target_seq.view(-1).long())\r\n",
        "    loss.backward() # Does backpropagation and calculates gradients\r\n",
        "    optimizer.step() # Updates the weights accordingly\r\n",
        "\r\n",
        "    epoch += 1\r\n",
        "    if epoch%10 == 0:\r\n",
        "        t2 = time.time()-t1\r\n",
        "        t1 = time.time()\r\n",
        "        print('Epoch: {}.............'.format(epoch), end=' ')\r\n",
        "        print(\"Loss: {:.4f}   {:.4f}\".format(loss.item(), t2))\r\n",
        "'''\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10/5000............. Loss: 3.0941   7.2661\n",
            "Epoch: 20/5000............. Loss: 3.0684   8.0244\n",
            "Epoch: 30/5000............. Loss: 3.0474   8.1433\n",
            "Epoch: 40/5000............. Loss: 3.0268   8.1981\n",
            "Epoch: 50/5000............. Loss: 3.0016   8.2507\n",
            "Epoch: 60/5000............. Loss: 2.9576   8.3461\n",
            "Epoch: 70/5000............. Loss: 2.8926   8.4706\n",
            "Epoch: 80/5000............. Loss: 2.8294   8.5195\n",
            "Epoch: 90/5000............. Loss: 2.7619   8.6317\n",
            "Epoch: 100/5000............. Loss: 2.6927   8.6881\n",
            "Epoch: 110/5000............. Loss: 2.6207   8.6541\n",
            "Epoch: 120/5000............. Loss: 2.6485   8.7073\n",
            "Epoch: 130/5000............. Loss: 2.5586   8.7747\n",
            "Epoch: 140/5000............. Loss: 2.4961   8.8484\n",
            "Epoch: 150/5000............. Loss: 2.4285   8.9106\n",
            "Epoch: 160/5000............. Loss: 2.3726   8.9602\n",
            "Epoch: 170/5000............. Loss: 2.3233   9.0520\n",
            "Epoch: 180/5000............. Loss: 2.2858   9.1469\n",
            "Epoch: 190/5000............. Loss: 2.2512   9.2762\n",
            "Epoch: 200/5000............. Loss: 2.1934   9.3621\n",
            "Epoch: 210/5000............. Loss: 2.1474   9.2250\n",
            "Epoch: 220/5000............. Loss: 2.1025   9.1270\n",
            "Epoch: 230/5000............. Loss: 2.1037   9.0862\n",
            "Epoch: 240/5000............. Loss: 2.0460   9.0623\n",
            "Epoch: 250/5000............. Loss: 1.9928   9.0388\n",
            "Epoch: 260/5000............. Loss: 1.9557   9.0861\n",
            "Epoch: 270/5000............. Loss: 1.8946   9.1101\n",
            "Epoch: 280/5000............. Loss: 1.8697   9.1612\n",
            "Epoch: 290/5000............. Loss: 1.8017   9.2425\n",
            "Epoch: 300/5000............. Loss: 1.7825   9.2892\n",
            "Epoch: 310/5000............. Loss: 1.7054   9.4131\n",
            "Epoch: 320/5000............. Loss: 1.6585   9.5526\n",
            "Epoch: 330/5000............. Loss: 1.5856   9.5664\n",
            "Epoch: 340/5000............. Loss: 1.5519   9.4203\n",
            "Epoch: 350/5000............. Loss: 1.4878   9.3261\n",
            "Epoch: 360/5000............. Loss: 1.4403   9.2835\n",
            "Epoch: 370/5000............. Loss: 1.3856   9.2642\n",
            "Epoch: 380/5000............. Loss: 1.3217   9.2800\n",
            "Epoch: 390/5000............. Loss: 1.3228   9.3350\n",
            "Epoch: 400/5000............. Loss: 1.2363   9.4056\n",
            "Epoch: 410/5000............. Loss: 1.1775   9.4928\n",
            "Epoch: 420/5000............. Loss: 1.1555   9.5155\n",
            "Epoch: 430/5000............. Loss: 1.1109   9.6632\n",
            "Epoch: 440/5000............. Loss: 1.0614   9.8770\n",
            "Epoch: 450/5000............. Loss: 1.0409   9.9519\n",
            "Epoch: 460/5000............. Loss: 0.9595   9.9954\n",
            "Epoch: 470/5000............. Loss: 0.9213   9.9285\n",
            "Epoch: 480/5000............. Loss: 0.8938   9.9313\n",
            "Epoch: 490/5000............. Loss: 0.8377   9.8446\n",
            "Epoch: 500/5000............. Loss: 0.8278   9.8253\n",
            "Epoch: 510/5000............. Loss: 0.7868   9.8131\n",
            "Epoch: 520/5000............. Loss: 0.7372   9.8758\n",
            "Epoch: 530/5000............. Loss: 0.6986   9.9090\n",
            "Epoch: 540/5000............. Loss: 0.6908   9.9246\n",
            "Epoch: 550/5000............. Loss: 0.6696   9.9262\n",
            "Epoch: 560/5000............. Loss: 0.6336   9.9371\n",
            "Epoch: 570/5000............. Loss: 0.5912   9.9018\n",
            "Epoch: 580/5000............. Loss: 0.5597   9.9092\n",
            "Epoch: 590/5000............. Loss: 0.5385   9.9308\n",
            "Epoch: 600/5000............. Loss: 0.5159   9.8900\n",
            "Epoch: 610/5000............. Loss: 0.4936   9.8899\n",
            "Epoch: 620/5000............. Loss: 0.4929   9.8857\n",
            "Epoch: 630/5000............. Loss: 0.4712   9.8962\n",
            "Epoch: 640/5000............. Loss: 0.4447   9.8924\n",
            "Epoch: 650/5000............. Loss: 0.4265   9.9029\n",
            "Epoch: 660/5000............. Loss: 0.4112   9.9055\n",
            "Epoch: 670/5000............. Loss: 0.3977   9.8886\n",
            "Epoch: 680/5000............. Loss: 0.3864   9.9060\n",
            "Epoch: 690/5000............. Loss: 0.3751   9.9006\n",
            "Epoch: 700/5000............. Loss: 0.3651   9.9080\n",
            "Epoch: 710/5000............. Loss: 0.3546   9.9197\n",
            "Epoch: 720/5000............. Loss: 0.3449   9.9036\n",
            "Epoch: 730/5000............. Loss: 0.3364   9.9293\n",
            "Epoch: 740/5000............. Loss: 0.3285   9.9017\n",
            "Epoch: 750/5000............. Loss: 0.3210   9.9200\n",
            "Epoch: 760/5000............. Loss: 0.3141   9.9086\n",
            "Epoch: 770/5000............. Loss: 0.3076   9.9264\n",
            "Epoch: 780/5000............. Loss: 0.3014   9.9419\n",
            "Epoch: 790/5000............. Loss: 0.2957   9.9165\n",
            "Epoch: 800/5000............. Loss: 0.2906   9.9403\n",
            "Epoch: 810/5000............. Loss: 0.2853   9.9194\n",
            "Epoch: 820/5000............. Loss: 0.2802   9.9194\n",
            "Epoch: 830/5000............. Loss: 0.2756   9.9064\n",
            "Epoch: 840/5000............. Loss: 0.2711   9.9090\n",
            "Epoch: 850/5000............. Loss: 0.2668   9.9510\n",
            "Epoch: 860/5000............. Loss: 0.2628   9.9570\n",
            "Epoch: 870/5000............. Loss: 0.2589   9.9139\n",
            "Epoch: 880/5000............. Loss: 0.2552   9.9371\n",
            "Epoch: 890/5000............. Loss: 0.2517   9.9312\n",
            "Epoch: 900/5000............. Loss: 0.2483   9.9296\n",
            "Epoch: 910/5000............. Loss: 0.2451   9.9145\n",
            "Epoch: 920/5000............. Loss: 0.2420   9.9273\n",
            "Epoch: 930/5000............. Loss: 0.2397   9.9174\n",
            "Epoch: 940/5000............. Loss: 0.2366   9.9024\n",
            "Epoch: 950/5000............. Loss: 0.2334   9.9060\n",
            "Epoch: 960/5000............. Loss: 0.2305   9.9006\n",
            "Epoch: 970/5000............. Loss: 0.2279   9.9376\n",
            "Epoch: 980/5000............. Loss: 0.2253   9.8911\n",
            "Epoch: 990/5000............. Loss: 0.2229   9.9056\n",
            "Epoch: 1000/5000............. Loss: 0.2205   9.9014\n",
            "Epoch: 1010/5000............. Loss: 0.2182   9.9030\n",
            "Epoch: 1020/5000............. Loss: 0.2160   9.9090\n",
            "Epoch: 1030/5000............. Loss: 0.2138   9.9060\n",
            "Epoch: 1040/5000............. Loss: 0.2117   9.9043\n",
            "Epoch: 1050/5000............. Loss: 0.2096   9.8627\n",
            "Epoch: 1060/5000............. Loss: 0.2076   9.9088\n",
            "Epoch: 1070/5000............. Loss: 0.2057   9.9266\n",
            "Epoch: 1080/5000............. Loss: 0.2050   9.9218\n",
            "Epoch: 1090/5000............. Loss: 0.2105   9.9027\n",
            "Epoch: 1100/5000............. Loss: 0.3580   9.9359\n",
            "Epoch: 1110/5000............. Loss: 3.1396   9.9276\n",
            "Epoch: 1120/5000............. Loss: 1.9631   9.9496\n",
            "Epoch: 1130/5000............. Loss: 1.6194   9.9364\n",
            "Epoch: 1140/5000............. Loss: 1.3378   9.9287\n",
            "Epoch: 1150/5000............. Loss: 1.1084   9.8973\n",
            "Epoch: 1160/5000............. Loss: 0.9179   9.8795\n",
            "Epoch: 1170/5000............. Loss: 0.7625   9.7789\n",
            "Epoch: 1180/5000............. Loss: 0.6377   9.7180\n",
            "Epoch: 1190/5000............. Loss: 0.5402   9.7287\n",
            "Epoch: 1200/5000............. Loss: 0.4665   9.7097\n",
            "Epoch: 1210/5000............. Loss: 0.4119   9.7754\n",
            "Epoch: 1220/5000............. Loss: 0.3717   9.9062\n",
            "Epoch: 1230/5000............. Loss: 0.3418   9.9494\n",
            "Epoch: 1240/5000............. Loss: 0.3192   9.9542\n",
            "Epoch: 1250/5000............. Loss: 0.3016   9.9023\n",
            "Epoch: 1260/5000............. Loss: 0.2876   9.7855\n",
            "Epoch: 1270/5000............. Loss: 0.2763   9.7362\n",
            "Epoch: 1280/5000............. Loss: 0.2668   9.6806\n",
            "Epoch: 1290/5000............. Loss: 0.2588   9.6977\n",
            "Epoch: 1300/5000............. Loss: 0.2519   9.7555\n",
            "Epoch: 1310/5000............. Loss: 0.2458   9.7998\n",
            "Epoch: 1320/5000............. Loss: 0.2405   9.8403\n",
            "Epoch: 1330/5000............. Loss: 0.2357   9.7439\n",
            "Epoch: 1340/5000............. Loss: 0.2314   9.7607\n",
            "Epoch: 1350/5000............. Loss: 0.2275   9.6943\n",
            "Epoch: 1360/5000............. Loss: 0.2239   9.6146\n",
            "Epoch: 1370/5000............. Loss: 0.2206   9.7065\n",
            "Epoch: 1380/5000............. Loss: 0.2176   9.7432\n",
            "Epoch: 1390/5000............. Loss: 0.2147   9.7389\n",
            "Epoch: 1400/5000............. Loss: 0.2120   9.7646\n",
            "Epoch: 1410/5000............. Loss: 0.2095   9.7511\n",
            "Epoch: 1420/5000............. Loss: 0.2072   9.7374\n",
            "Epoch: 1430/5000............. Loss: 0.2050   9.8008\n",
            "Epoch: 1440/5000............. Loss: 0.2029   9.8729\n",
            "Epoch: 1450/5000............. Loss: 0.2009   9.9408\n",
            "Epoch: 1460/5000............. Loss: 0.1990   9.9472\n",
            "Epoch: 1470/5000............. Loss: 0.1971   9.9226\n",
            "Epoch: 1480/5000............. Loss: 0.1954   9.8955\n",
            "Epoch: 1490/5000............. Loss: 0.1937   9.8817\n",
            "Epoch: 1500/5000............. Loss: 0.1921   9.8268\n",
            "Epoch: 1510/5000............. Loss: 0.1906   9.8270\n",
            "Epoch: 1520/5000............. Loss: 0.1891   9.8276\n",
            "Epoch: 1530/5000............. Loss: 0.1877   9.8554\n",
            "Epoch: 1540/5000............. Loss: 0.1863   9.8874\n",
            "Epoch: 1550/5000............. Loss: 0.1850   9.9510\n",
            "Epoch: 1560/5000............. Loss: 0.1837   9.9307\n",
            "Epoch: 1570/5000............. Loss: 0.1824   9.9404\n",
            "Epoch: 1580/5000............. Loss: 0.1812   9.9399\n",
            "Epoch: 1590/5000............. Loss: 0.1800   9.9159\n",
            "Epoch: 1600/5000............. Loss: 0.1789   9.9203\n",
            "Epoch: 1610/5000............. Loss: 0.1778   9.9332\n",
            "Epoch: 1620/5000............. Loss: 0.1767   9.9152\n",
            "Epoch: 1630/5000............. Loss: 0.1756   9.9089\n",
            "Epoch: 1640/5000............. Loss: 0.1746   9.9006\n",
            "Epoch: 1650/5000............. Loss: 0.1736   9.9094\n",
            "Epoch: 1660/5000............. Loss: 0.1726   9.8853\n",
            "Epoch: 1670/5000............. Loss: 0.1716   9.9023\n",
            "Epoch: 1680/5000............. Loss: 0.1707   9.9093\n",
            "Epoch: 1690/5000............. Loss: 0.1698   9.9128\n",
            "Epoch: 1700/5000............. Loss: 0.1689   9.9227\n",
            "Epoch: 1710/5000............. Loss: 0.1680   9.9142\n",
            "Epoch: 1720/5000............. Loss: 0.1671   9.9339\n",
            "Epoch: 1730/5000............. Loss: 0.1663   9.8997\n",
            "Epoch: 1740/5000............. Loss: 0.1655   9.9056\n",
            "Epoch: 1750/5000............. Loss: 0.1647   9.9273\n",
            "Epoch: 1760/5000............. Loss: 0.1639   9.9184\n",
            "Epoch: 1770/5000............. Loss: 0.1631   9.9733\n",
            "Epoch: 1780/5000............. Loss: 0.1623   9.8945\n",
            "Epoch: 1790/5000............. Loss: 0.1616   9.9410\n",
            "Epoch: 1800/5000............. Loss: 0.1608   9.8813\n",
            "Epoch: 1810/5000............. Loss: 0.1601   9.8895\n",
            "Epoch: 1820/5000............. Loss: 0.1594   9.8922\n",
            "Epoch: 1830/5000............. Loss: 0.1587   9.8566\n",
            "Epoch: 1840/5000............. Loss: 0.1580   9.8851\n",
            "Epoch: 1850/5000............. Loss: 0.1573   9.8950\n",
            "Epoch: 1860/5000............. Loss: 0.1566   9.8561\n",
            "Epoch: 1870/5000............. Loss: 0.1560   9.8042\n",
            "Epoch: 1880/5000............. Loss: 0.1553   9.7642\n",
            "Epoch: 1890/5000............. Loss: 0.1547   9.6958\n",
            "Epoch: 1900/5000............. Loss: 0.1540   9.6941\n",
            "Epoch: 1910/5000............. Loss: 0.1534   9.6532\n",
            "Epoch: 1920/5000............. Loss: 0.1528   9.7284\n",
            "Epoch: 1930/5000............. Loss: 0.1522   9.7602\n",
            "Epoch: 1940/5000............. Loss: 0.1516   9.7475\n",
            "Epoch: 1950/5000............. Loss: 0.1510   9.7019\n",
            "Epoch: 1960/5000............. Loss: 0.1504   9.5508\n",
            "Epoch: 1970/5000............. Loss: 0.1498   9.4756\n",
            "Epoch: 1980/5000............. Loss: 0.1493   9.4023\n",
            "Epoch: 1990/5000............. Loss: 0.1487   9.4781\n",
            "Epoch: 2000/5000............. Loss: 0.1482   9.4730\n",
            "Epoch: 2010/5000............. Loss: 0.1476   9.5332\n",
            "Epoch: 2020/5000............. Loss: 0.1471   9.5182\n",
            "Epoch: 2030/5000............. Loss: 0.1466   9.5193\n",
            "Epoch: 2040/5000............. Loss: 0.1460   9.5149\n",
            "Epoch: 2050/5000............. Loss: 0.1455   9.4448\n",
            "Epoch: 2060/5000............. Loss: 0.1450   9.4277\n",
            "Epoch: 2070/5000............. Loss: 0.1445   9.4430\n",
            "Epoch: 2080/5000............. Loss: 0.1440   9.3873\n",
            "Epoch: 2090/5000............. Loss: 0.1435   9.3977\n",
            "Epoch: 2100/5000............. Loss: 0.1430   9.4361\n",
            "Epoch: 2110/5000............. Loss: 0.1425   9.4440\n",
            "Epoch: 2120/5000............. Loss: 0.1420   9.4962\n",
            "Epoch: 2130/5000............. Loss: 0.1416   9.5149\n",
            "Epoch: 2140/5000............. Loss: 0.1411   9.5524\n",
            "Epoch: 2150/5000............. Loss: 0.1406   9.5357\n",
            "Epoch: 2160/5000............. Loss: 0.1402   9.5016\n",
            "Epoch: 2170/5000............. Loss: 0.1397   9.5452\n",
            "Epoch: 2180/5000............. Loss: 0.1392   9.6770\n",
            "Epoch: 2190/5000............. Loss: 0.1388   9.7931\n",
            "Epoch: 2200/5000............. Loss: 0.1384   9.8073\n",
            "Epoch: 2210/5000............. Loss: 0.1379   9.7642\n",
            "Epoch: 2220/5000............. Loss: 0.1375   9.7037\n",
            "Epoch: 2230/5000............. Loss: 0.1371   9.7106\n",
            "Epoch: 2240/5000............. Loss: 0.1366   9.6503\n",
            "Epoch: 2250/5000............. Loss: 0.1362   9.6708\n",
            "Epoch: 2260/5000............. Loss: 0.1358   9.7392\n",
            "Epoch: 2270/5000............. Loss: 0.1354   9.7235\n",
            "Epoch: 2280/5000............. Loss: 0.1350   9.7333\n",
            "Epoch: 2290/5000............. Loss: 0.1346   9.7536\n",
            "Epoch: 2300/5000............. Loss: 0.1342   9.7510\n",
            "Epoch: 2310/5000............. Loss: 0.1338   9.7647\n",
            "Epoch: 2320/5000............. Loss: 0.1334   9.7560\n",
            "Epoch: 2330/5000............. Loss: 0.1330   9.7982\n",
            "Epoch: 2340/5000............. Loss: 0.1326   9.7911\n",
            "Epoch: 2350/5000............. Loss: 0.1322   9.8713\n",
            "Epoch: 2360/5000............. Loss: 0.1318   9.8510\n",
            "Epoch: 2370/5000............. Loss: 0.1314   9.8062\n",
            "Epoch: 2380/5000............. Loss: 0.1311   9.7271\n",
            "Epoch: 2390/5000............. Loss: 0.1307   9.6836\n",
            "Epoch: 2400/5000............. Loss: 0.1303   9.5420\n",
            "Epoch: 2410/5000............. Loss: 0.1300   9.5403\n",
            "Epoch: 2420/5000............. Loss: 0.1296   9.5551\n",
            "Epoch: 2430/5000............. Loss: 0.1292   9.5423\n",
            "Epoch: 2440/5000............. Loss: 0.1289   9.5774\n",
            "Epoch: 2450/5000............. Loss: 0.1285   9.7834\n",
            "Epoch: 2460/5000............. Loss: 0.1282   9.8208\n",
            "Epoch: 2470/5000............. Loss: 0.1278   9.7385\n",
            "Epoch: 2480/5000............. Loss: 0.1275   9.6550\n",
            "Epoch: 2490/5000............. Loss: 0.1271   9.5611\n",
            "Epoch: 2500/5000............. Loss: 0.1268   9.4873\n",
            "Epoch: 2510/5000............. Loss: 0.1265   9.4405\n",
            "Epoch: 2520/5000............. Loss: 0.1261   9.4389\n",
            "Epoch: 2530/5000............. Loss: 0.1258   9.4954\n",
            "Epoch: 2540/5000............. Loss: 0.1255   9.4966\n",
            "Epoch: 2550/5000............. Loss: 0.1251   9.5224\n",
            "Epoch: 2560/5000............. Loss: 0.1248   9.5300\n",
            "Epoch: 2570/5000............. Loss: 0.1245   9.5519\n",
            "Epoch: 2580/5000............. Loss: 0.1242   9.5755\n",
            "Epoch: 2590/5000............. Loss: 0.1238   9.7290\n",
            "Epoch: 2600/5000............. Loss: 0.1235   9.7295\n",
            "Epoch: 2610/5000............. Loss: 0.1232   9.6569\n",
            "Epoch: 2620/5000............. Loss: 0.1229   9.5857\n",
            "Epoch: 2630/5000............. Loss: 0.1226   9.4883\n",
            "Epoch: 2640/5000............. Loss: 0.1223   9.4755\n",
            "Epoch: 2650/5000............. Loss: 0.1220   9.5044\n",
            "Epoch: 2660/5000............. Loss: 0.1217   9.5370\n",
            "Epoch: 2670/5000............. Loss: 0.1214   9.5489\n",
            "Epoch: 2680/5000............. Loss: 0.1211   9.6510\n",
            "Epoch: 2690/5000............. Loss: 0.1208   9.9439\n",
            "Epoch: 2700/5000............. Loss: 0.1205   10.1152\n",
            "Epoch: 2710/5000............. Loss: 0.1202   10.0582\n",
            "Epoch: 2720/5000............. Loss: 0.1199   9.9470\n",
            "Epoch: 2730/5000............. Loss: 0.1196   9.8697\n",
            "Epoch: 2740/5000............. Loss: 0.1193   9.8225\n",
            "Epoch: 2750/5000............. Loss: 0.1191   9.7871\n",
            "Epoch: 2760/5000............. Loss: 0.1188   9.8618\n",
            "Epoch: 2770/5000............. Loss: 0.1185   9.8872\n",
            "Epoch: 2780/5000............. Loss: 0.1182   9.8946\n",
            "Epoch: 2790/5000............. Loss: 0.1179   9.8739\n",
            "Epoch: 2800/5000............. Loss: 0.1177   9.8489\n",
            "Epoch: 2810/5000............. Loss: 0.1174   9.8067\n",
            "Epoch: 2820/5000............. Loss: 0.1171   9.7540\n",
            "Epoch: 2830/5000............. Loss: 0.1168   9.7187\n",
            "Epoch: 2840/5000............. Loss: 0.1166   9.6919\n",
            "Epoch: 2850/5000............. Loss: 0.1163   9.5464\n",
            "Epoch: 2860/5000............. Loss: 0.1160   9.5066\n",
            "Epoch: 2870/5000............. Loss: 0.1158   9.4757\n",
            "Epoch: 2880/5000............. Loss: 0.1155   9.3786\n",
            "Epoch: 2890/5000............. Loss: 0.1153   9.3955\n",
            "Epoch: 2900/5000............. Loss: 0.1150   9.4353\n",
            "Epoch: 2910/5000............. Loss: 0.1147   9.4133\n",
            "Epoch: 2920/5000............. Loss: 0.1145   9.3770\n",
            "Epoch: 2930/5000............. Loss: 0.1142   9.3743\n",
            "Epoch: 2940/5000............. Loss: 0.1140   9.3370\n",
            "Epoch: 2950/5000............. Loss: 0.1137   9.3470\n",
            "Epoch: 2960/5000............. Loss: 0.1135   9.3760\n",
            "Epoch: 2970/5000............. Loss: 0.1132   9.3111\n",
            "Epoch: 2980/5000............. Loss: 0.1130   9.3235\n",
            "Epoch: 2990/5000............. Loss: 0.1128   9.3684\n",
            "Epoch: 3000/5000............. Loss: 0.1125   9.3534\n",
            "Epoch: 3010/5000............. Loss: 0.1123   9.3229\n",
            "Epoch: 3020/5000............. Loss: 0.1120   9.3143\n",
            "Epoch: 3030/5000............. Loss: 0.1118   9.3329\n",
            "Epoch: 3040/5000............. Loss: 0.1116   9.3420\n",
            "Epoch: 3050/5000............. Loss: 0.1113   9.2920\n",
            "Epoch: 3060/5000............. Loss: 0.1111   9.3268\n",
            "Epoch: 3070/5000............. Loss: 0.1108   9.3161\n",
            "Epoch: 3080/5000............. Loss: 0.1106   9.3214\n",
            "Epoch: 3090/5000............. Loss: 0.1104   9.3001\n",
            "Epoch: 3100/5000............. Loss: 0.1102   9.3144\n",
            "Epoch: 3110/5000............. Loss: 0.1099   9.3049\n",
            "Epoch: 3120/5000............. Loss: 0.1097   9.3190\n",
            "Epoch: 3130/5000............. Loss: 0.1095   9.3148\n",
            "Epoch: 3140/5000............. Loss: 0.1093   9.3379\n",
            "Epoch: 3150/5000............. Loss: 0.1090   9.3059\n",
            "Epoch: 3160/5000............. Loss: 0.1088   9.2834\n",
            "Epoch: 3170/5000............. Loss: 0.1086   9.3112\n",
            "Epoch: 3180/5000............. Loss: 0.1084   9.3151\n",
            "Epoch: 3190/5000............. Loss: 0.1081   9.3304\n",
            "Epoch: 3200/5000............. Loss: 0.1079   9.3102\n",
            "Epoch: 3210/5000............. Loss: 0.1077   9.3829\n",
            "Epoch: 3220/5000............. Loss: 0.1075   9.5595\n",
            "Epoch: 3230/5000............. Loss: 0.1073   9.7142\n",
            "Epoch: 3240/5000............. Loss: 0.1071   9.7057\n",
            "Epoch: 3250/5000............. Loss: 0.1069   9.5869\n",
            "Epoch: 3260/5000............. Loss: 0.1067   9.4952\n",
            "Epoch: 3270/5000............. Loss: 0.1064   9.4159\n",
            "Epoch: 3280/5000............. Loss: 0.1062   9.3401\n",
            "Epoch: 3290/5000............. Loss: 0.1060   9.4202\n",
            "Epoch: 3300/5000............. Loss: 0.1058   9.4616\n",
            "Epoch: 3310/5000............. Loss: 0.1056   9.5175\n",
            "Epoch: 3320/5000............. Loss: 0.1054   9.5388\n",
            "Epoch: 3330/5000............. Loss: 0.1052   9.5163\n",
            "Epoch: 3340/5000............. Loss: 0.1050   9.4843\n",
            "Epoch: 3350/5000............. Loss: 0.1048   9.4708\n",
            "Epoch: 3360/5000............. Loss: 0.1046   9.3971\n",
            "Epoch: 3370/5000............. Loss: 0.1044   9.4504\n",
            "Epoch: 3380/5000............. Loss: 0.1042   9.4288\n",
            "Epoch: 3390/5000............. Loss: 0.1040   9.5752\n",
            "Epoch: 3400/5000............. Loss: 0.1038   9.7440\n",
            "Epoch: 3410/5000............. Loss: 0.1036   9.6944\n",
            "Epoch: 3420/5000............. Loss: 0.1034   9.5972\n",
            "Epoch: 3430/5000............. Loss: 0.1033   9.4825\n",
            "Epoch: 3440/5000............. Loss: 0.1031   9.3575\n",
            "Epoch: 3450/5000............. Loss: 0.1029   9.3506\n",
            "Epoch: 3460/5000............. Loss: 0.1027   9.3409\n",
            "Epoch: 3470/5000............. Loss: 0.1025   9.5271\n",
            "Epoch: 3480/5000............. Loss: 0.1023   9.8124\n",
            "Epoch: 3490/5000............. Loss: 0.1021   9.8457\n",
            "Epoch: 3500/5000............. Loss: 0.1019   9.6774\n",
            "Epoch: 3510/5000............. Loss: 0.1017   9.5735\n",
            "Epoch: 3520/5000............. Loss: 0.1016   9.4528\n",
            "Epoch: 3530/5000............. Loss: 0.1014   9.3992\n",
            "Epoch: 3540/5000............. Loss: 0.1012   9.4027\n",
            "Epoch: 3550/5000............. Loss: 0.1010   9.4208\n",
            "Epoch: 3560/5000............. Loss: 0.1008   9.4860\n",
            "Epoch: 3570/5000............. Loss: 0.1007   9.5358\n",
            "Epoch: 3580/5000............. Loss: 0.1005   9.5155\n",
            "Epoch: 3590/5000............. Loss: 0.1003   9.5294\n",
            "Epoch: 3600/5000............. Loss: 0.1001   9.5648\n",
            "Epoch: 3610/5000............. Loss: 0.1000   9.6513\n",
            "Epoch: 3620/5000............. Loss: 0.0998   9.6830\n",
            "Epoch: 3630/5000............. Loss: 0.0996   9.7960\n",
            "Epoch: 3640/5000............. Loss: 0.0994   9.8359\n",
            "Epoch: 3650/5000............. Loss: 0.0993   9.8323\n",
            "Epoch: 3660/5000............. Loss: 0.0991   9.7448\n",
            "Epoch: 3670/5000............. Loss: 0.0989   9.6832\n",
            "Epoch: 3680/5000............. Loss: 0.0987   9.7092\n",
            "Epoch: 3690/5000............. Loss: 0.0986   9.7187\n",
            "Epoch: 3700/5000............. Loss: 0.0984   9.8163\n",
            "Epoch: 3710/5000............. Loss: 0.0982   9.7878\n",
            "Epoch: 3720/5000............. Loss: 0.0981   9.7117\n",
            "Epoch: 3730/5000............. Loss: 0.0979   9.6582\n",
            "Epoch: 3740/5000............. Loss: 0.0977   9.5265\n",
            "Epoch: 3750/5000............. Loss: 0.0976   9.4834\n",
            "Epoch: 3760/5000............. Loss: 0.0974   9.4837\n",
            "Epoch: 3770/5000............. Loss: 0.0972   9.5116\n",
            "Epoch: 3780/5000............. Loss: 0.0971   9.5502\n",
            "Epoch: 3790/5000............. Loss: 0.0969   9.5772\n",
            "Epoch: 3800/5000............. Loss: 0.0968   9.5664\n",
            "Epoch: 3810/5000............. Loss: 0.0966   9.5332\n",
            "Epoch: 3820/5000............. Loss: 0.0964   9.5335\n",
            "Epoch: 3830/5000............. Loss: 0.0963   9.4623\n",
            "Epoch: 3840/5000............. Loss: 0.0961   9.4299\n",
            "Epoch: 3850/5000............. Loss: 0.0960   9.4021\n",
            "Epoch: 3860/5000............. Loss: 0.0958   9.4292\n",
            "Epoch: 3870/5000............. Loss: 0.0957   9.3696\n",
            "Epoch: 3880/5000............. Loss: 0.0955   9.3882\n",
            "Epoch: 3890/5000............. Loss: 0.0953   9.4126\n",
            "Epoch: 3900/5000............. Loss: 0.0952   9.4376\n",
            "Epoch: 3910/5000............. Loss: 0.0950   9.4208\n",
            "Epoch: 3920/5000............. Loss: 0.0949   9.4481\n",
            "Epoch: 3930/5000............. Loss: 0.0947   9.5011\n",
            "Epoch: 3940/5000............. Loss: 0.0946   9.4639\n",
            "Epoch: 3950/5000............. Loss: 0.0944   9.5076\n",
            "Epoch: 3960/5000............. Loss: 0.0943   9.6702\n",
            "Epoch: 3970/5000............. Loss: 0.0941   9.7160\n",
            "Epoch: 3980/5000............. Loss: 0.0940   9.6836\n",
            "Epoch: 3990/5000............. Loss: 0.0938   9.6109\n",
            "Epoch: 4000/5000............. Loss: 0.0937   9.5233\n",
            "Epoch: 4010/5000............. Loss: 0.0935   9.5113\n",
            "Epoch: 4020/5000............. Loss: 0.0934   9.4537\n",
            "Epoch: 4030/5000............. Loss: 0.0933   9.5090\n",
            "Epoch: 4040/5000............. Loss: 0.0931   9.5243\n",
            "Epoch: 4050/5000............. Loss: 0.0930   9.5498\n",
            "Epoch: 4060/5000............. Loss: 0.0928   9.6084\n",
            "Epoch: 4070/5000............. Loss: 0.0927   9.6347\n",
            "Epoch: 4080/5000............. Loss: 0.0925   9.5692\n",
            "Epoch: 4090/5000............. Loss: 0.0924   9.5454\n",
            "Epoch: 4100/5000............. Loss: 0.0923   9.5530\n",
            "Epoch: 4110/5000............. Loss: 0.0921   9.5399\n",
            "Epoch: 4120/5000............. Loss: 0.0920   9.5390\n",
            "Epoch: 4130/5000............. Loss: 0.0918   9.5248\n",
            "Epoch: 4140/5000............. Loss: 0.0917   9.5429\n",
            "Epoch: 4150/5000............. Loss: 0.0916   9.5415\n",
            "Epoch: 4160/5000............. Loss: 0.0914   9.5450\n",
            "Epoch: 4170/5000............. Loss: 0.0913   9.5393\n",
            "Epoch: 4180/5000............. Loss: 0.0911   9.5149\n",
            "Epoch: 4190/5000............. Loss: 0.0910   9.5232\n",
            "Epoch: 4200/5000............. Loss: 0.0909   9.5246\n",
            "Epoch: 4210/5000............. Loss: 0.0907   9.5380\n",
            "Epoch: 4220/5000............. Loss: 0.0906   9.6502\n",
            "Epoch: 4230/5000............. Loss: 0.0905   9.8018\n",
            "Epoch: 4240/5000............. Loss: 0.0903   9.8541\n",
            "Epoch: 4250/5000............. Loss: 0.0902   9.8319\n",
            "Epoch: 4260/5000............. Loss: 0.0901   9.7078\n",
            "Epoch: 4270/5000............. Loss: 0.0899   9.5863\n",
            "Epoch: 4280/5000............. Loss: 0.0898   9.5414\n",
            "Epoch: 4290/5000............. Loss: 0.0897   9.7299\n",
            "Epoch: 4300/5000............. Loss: 0.0896   9.7719\n",
            "Epoch: 4310/5000............. Loss: 0.0894   9.8412\n",
            "Epoch: 4320/5000............. Loss: 0.0893   9.8204\n",
            "Epoch: 4330/5000............. Loss: 0.0892   9.7525\n",
            "Epoch: 4340/5000............. Loss: 0.0890   9.7060\n",
            "Epoch: 4350/5000............. Loss: 0.0889   9.6102\n",
            "Epoch: 4360/5000............. Loss: 0.0888   9.5448\n",
            "Epoch: 4370/5000............. Loss: 0.0887   9.5337\n",
            "Epoch: 4380/5000............. Loss: 0.0885   9.6765\n",
            "Epoch: 4390/5000............. Loss: 0.0884   9.8045\n",
            "Epoch: 4400/5000............. Loss: 0.0883   9.8644\n",
            "Epoch: 4410/5000............. Loss: 0.0882   9.8708\n",
            "Epoch: 4420/5000............. Loss: 0.0880   9.7599\n",
            "Epoch: 4430/5000............. Loss: 0.0879   9.7008\n",
            "Epoch: 4440/5000............. Loss: 0.0878   9.7137\n",
            "Epoch: 4450/5000............. Loss: 0.0877   9.7240\n",
            "Epoch: 4460/5000............. Loss: 0.0875   9.7304\n",
            "Epoch: 4470/5000............. Loss: 0.0874   9.7239\n",
            "Epoch: 4480/5000............. Loss: 0.0873   9.6870\n",
            "Epoch: 4490/5000............. Loss: 0.0872   9.5517\n",
            "Epoch: 4500/5000............. Loss: 0.0871   9.5506\n",
            "Epoch: 4510/5000............. Loss: 0.0869   9.4357\n",
            "Epoch: 4520/5000............. Loss: 0.0868   9.4858\n",
            "Epoch: 4530/5000............. Loss: 0.0867   9.5095\n",
            "Epoch: 4540/5000............. Loss: 0.0866   9.5221\n",
            "Epoch: 4550/5000............. Loss: 0.0865   9.5743\n",
            "Epoch: 4560/5000............. Loss: 0.0864   9.5118\n",
            "Epoch: 4570/5000............. Loss: 0.0862   9.5368\n",
            "Epoch: 4580/5000............. Loss: 0.0861   9.5196\n",
            "Epoch: 4590/5000............. Loss: 0.0860   9.5045\n",
            "Epoch: 4600/5000............. Loss: 0.0859   9.5184\n",
            "Epoch: 4610/5000............. Loss: 0.0858   9.5092\n",
            "Epoch: 4620/5000............. Loss: 0.0857   9.5478\n",
            "Epoch: 4630/5000............. Loss: 0.0856   9.4804\n",
            "Epoch: 4640/5000............. Loss: 0.0854   9.4737\n",
            "Epoch: 4650/5000............. Loss: 0.0853   9.4838\n",
            "Epoch: 4660/5000............. Loss: 0.0852   9.4511\n",
            "Epoch: 4670/5000............. Loss: 0.0851   9.4726\n",
            "Epoch: 4680/5000............. Loss: 0.0850   9.4344\n",
            "Epoch: 4690/5000............. Loss: 0.0849   9.4701\n",
            "Epoch: 4700/5000............. Loss: 0.0848   9.4478\n",
            "Epoch: 4710/5000............. Loss: 0.0847   9.4253\n",
            "Epoch: 4720/5000............. Loss: 0.0846   9.4862\n",
            "Epoch: 4730/5000............. Loss: 0.0845   9.4198\n",
            "Epoch: 4740/5000............. Loss: 0.0843   9.4501\n",
            "Epoch: 4750/5000............. Loss: 0.0842   9.4421\n",
            "Epoch: 4760/5000............. Loss: 0.0841   9.4491\n",
            "Epoch: 4770/5000............. Loss: 0.0840   9.5133\n",
            "Epoch: 4780/5000............. Loss: 0.0839   9.4118\n",
            "Epoch: 4790/5000............. Loss: 0.0838   9.4626\n",
            "Epoch: 4800/5000............. Loss: 0.0837   9.3883\n",
            "Epoch: 4810/5000............. Loss: 0.0836   9.4305\n",
            "Epoch: 4820/5000............. Loss: 0.0835   9.5615\n",
            "Epoch: 4830/5000............. Loss: 0.0834   9.6677\n",
            "Epoch: 4840/5000............. Loss: 0.0833   9.7982\n",
            "Epoch: 4850/5000............. Loss: 0.0832   9.7232\n",
            "Epoch: 4860/5000............. Loss: 0.0831   9.7518\n",
            "Epoch: 4870/5000............. Loss: 0.0830   9.8024\n",
            "Epoch: 4880/5000............. Loss: 0.0829   9.6858\n",
            "Epoch: 4890/5000............. Loss: 0.0828   9.6183\n",
            "Epoch: 4900/5000............. Loss: 0.0827   9.5113\n",
            "Epoch: 4910/5000............. Loss: 0.0826   9.3797\n",
            "Epoch: 4920/5000............. Loss: 0.0825   9.3211\n",
            "Epoch: 4930/5000............. Loss: 0.0824   9.3259\n",
            "Epoch: 4940/5000............. Loss: 0.0823   9.3839\n",
            "Epoch: 4950/5000............. Loss: 0.0822   9.4512\n",
            "Epoch: 4960/5000............. Loss: 0.0821   9.4596\n",
            "Epoch: 4970/5000............. Loss: 0.0820   9.4784\n",
            "Epoch: 4980/5000............. Loss: 0.0819   9.4136\n",
            "Epoch: 4990/5000............. Loss: 0.0818   9.3158\n",
            "Epoch: 5000/5000............. Loss: 0.0817   9.3257\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nloss = 1000\\nepoch = 1\\nt1 = time.time()\\nwhile loss > 0.3:\\n    optimizer.zero_grad() # Clears existing gradients from previous epoch\\n    input_sample, target_sample = sample_seq(batch_len, batch_size, input_seq, target_seq)\\n    output, hidden = model(input_seq)\\n    loss = criterion(output, target_seq.view(-1).long())\\n    loss.backward() # Does backpropagation and calculates gradients\\n    optimizer.step() # Updates the weights accordingly\\n\\n    epoch += 1\\n    if epoch%10 == 0:\\n        t2 = time.time()-t1\\n        t1 = time.time()\\n        print(\\'Epoch: {}.............\\'.format(epoch), end=\\' \\')\\n        print(\"Loss: {:.4f}   {:.4f}\".format(loss.item(), t2))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "aLxARabPe-_j",
        "outputId": "8c62c005-521e-46d5-ff42-298c767f2ac8"
      },
      "source": [
        "test_txt = \"Les petits pois de la grande-bretagne\"\r\n",
        "sample(model, 600, test_txt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Les petits pois de la grande-bretagne ene pouss nocmentant disquilie est en ie leur daine da semporaprdée à sa ter tomoe loide du plus grand des satellites planétaires par rapport à la taille de la planète autour de laquelle il orbite. Elle est le deuxième satellite le plus dense du Système solaire après Io, un satellite de Jupiter. La Lune est en rotation synchrone avec la Terre, lui montrant donc constamment la même face. Celle-ci, appelée face visible, est marquée par des mers lunaires volcaniques sombres qui remplissent les espaces entre les hautes terres claires — certaines atteignant 9 k'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}